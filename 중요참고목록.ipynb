{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <center>Major Reference</center>\n",
    "### 1. End to End Machine Learning Project\n",
    "###### (02_Hands on_Géron\\02_end_to_end_machine_learning_project.ipynb)\n",
    " _California housing price data_\n",
    "1. Look at the big picture.\n",
    "     - Frame the Problem\n",
    "     - Select a Performance Measure : **RMSE, MSE, MAE**\n",
    "     - Check the Assumptions\n",
    "2. Get the data.\n",
    "    - Create the Workspace\n",
    "    - Download the Data\n",
    "    - Take a Quick Look at the Data Structure\n",
    "    - Create a **Test Set**\n",
    "3. Discover and visualize the data to gain insights.\n",
    "    - Visualizing Geographical Data\n",
    "    - Looking for **Correlations**\n",
    "    - Experimenting with **Attribute Combinations**\n",
    "4. Prepare the **data** for Machine Learning **algorithms**.\n",
    "    - Data Cleaning\n",
    "    - Handling Text and Categorical Attributes\n",
    "    - Custom Transformers\n",
    "    - Feature Scaling\n",
    "    - Transformation **Pipelines**\n",
    "\n",
    "    Write **functions** for your Machine Learning algorithms, for several good reasons:\n",
    "        - This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset). You will gradually build a library of transformation functions that you can reuse\n",
    "in future projects.\n",
    "        - You can use these functions in your live system to transform the new data before\n",
    "feeding it to your algorithms.\n",
    "        - This will make it possible for you to easily try various transformations and see\n",
    "which combination of transformations works best.\n",
    "\n",
    "5. Select a model and **train** it.\n",
    "    - Training and Evaluating on the Training Set\n",
    "    - Better Evaluation Using **Cross-Validation**\n",
    "6. **Fine-tune** your model.\n",
    "    - Grid Search : the best combination of **hyperparameter** values for the RandomForestRegressor\n",
    "    - Randomized Search\n",
    "    - **Ensemble** Methods : try to combine the models that perform best\n",
    "    - Analyze the Best Models and Their Errors\n",
    "    - Evaluate Your System on the Test Set\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "---\n",
    "### **2. Hands on..._Géron**\n",
    "\n",
    "* _Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT)_ (p203)\n",
    "\n",
    "* Neural Network\n",
    "    - Chapter 10. Introduction to Artificial Neural Networks with Keras -\n",
    "Regression MLPs\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### **3. Keras Deep Learing** [케라스 딥러닝](https://tensorflow.blog/%ec%bc%80%eb%9d%bc%ec%8a%a4-%eb%94%a5%eb%9f%ac%eb%8b%9d/)\n",
    "\n",
    "- Chpapter 1 : Deep Learning 이란 ? 정의, 역사, 왜 해야하나...\n",
    "- Chapter 2 : 시작하기 전에: 신경망의 수학적 구성 요소\n",
    "    - 신경망의 엔진: 그래디언트 기반 최적화\n",
    "- Chapter 3, Section 6 : Predicting house prices: **a regression example**\n",
    "\n",
    "\n",
    "2016년과 2017년 캐글에는 그래디언트 부스팅 머신과 딥러닝의 두 가지 접근 방법이 주류를 이루었습니다. 특히 그래디언트 부스팅은\n",
    "구조적인 데이터인 경우에 사용되고, 딥러닝은 이미지 분류 같은 지각에 관한 문제에 사용됩니다. 전자의 경우 거의 항상 XGBoost\n",
    "라이브러리를 사용합니다. 이 라이브러리는 데이터 과학 분야에서 가장 인기 있는 두 언어인 파이썬Python과 R을 지원합니다.\n",
    "딥러닝을 사용하는 대부분의 참가자는 사용하기 쉽고, 유연하며 파이썬을 지원하는 케라스 라이브러리를 사용합니다.\n",
    "\n",
    "\n",
    "1.3.3 알고리즘\n",
    "하드웨어와 데이터에 이어 2000년대 후반까지는 매우 깊은 심층 신경망을 훈련시킬 수\n",
    "있는 안정적인 방법을 찾지 못했습니다. 이런 이유로 하나 또는 2개의 층만 사용하는 매우\n",
    "얕은 신경망만 가능했습니다. SVM과 랜덤 포레스트처럼 잘 훈련된 얕은 학습 방법에 비해\n",
    "크게 빛을 보지 못했습니다. 깊게 쌓은 층을 통과해서 그래디언트 gradient 를 전파하는\n",
    "것이 가장 문제였습니다. 신경망을 훈련하기 위한 피드백 신호가 층이 늘어남에 따라\n",
    "희미해지기 때문입니다. 2009~2010년경에 몇 가지 간단하지만 중요한 알고리즘이 개선되면서\n",
    "그래디언트를 더 잘 전파되게 만들어 주었고 상황이 바뀌었습니다.\n",
    "\n",
    "    * 신경망의 층에 더 잘 맞는 활성화 함수 activation function\n",
    "    * 층별 사전 훈련 pretraining을 불필요하게 만든 가중치 초기화 weight initialization 방법\n",
    "    * RMSProp과 Adam 같은 더 좋은 최적화 방법\n",
    "\n",
    "이런 기술의 향상으로 10개 이상의 층을 가진 모델을 훈련시킬 수 있게 되었을 때 비로소\n",
    "딥러닝이 빛을 발하기 시작했습니다. 2014~2016년 사이에 그래디언트를 더욱 잘 전파할 수 있는\n",
    "    * 배치 정규화 batch normalization,\n",
    "    * 잔차 연결residual connection,\n",
    "    * 깊이별 분리 합성곱depthwise separable convolution\n",
    "\n",
    "같은 고급 기술들이 개발되었습니다. 요즘에는 층의 깊이가 수천 개인 모델을 처음부터 훈련시킬 수 있습니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **4. ML with python**\n",
    "\n",
    "- _Ensembles of Decision Trees_ (p 203) : random forest and\n",
    "gradient boosted decision trees\n",
    "\n",
    "\n",
    "-  Representing Data and Engineering Features\n",
    "\n",
    "    Categorical Variables 212\n",
    "    One-Hot-Encoding (Dummy Variables) 213\n",
    "    Numbers Can Encode Categoricals 218\n",
    "    Binning, Discretization, Linear Models, and Trees 220\n",
    "    Interactions and Polynomials 224\n",
    "    Univariate Nonlinear Transformations 232\n",
    "    Automatic Feature Selection 236\n",
    "    Univariate Statistics 236\n",
    "    Model-Based Feature Selection 238\n",
    "    Iterative Feature Selection 240\n",
    "    Utilizing Expert Knowledge 242\n",
    "    Summary and Outlook 250\n",
    "\n",
    "5. Model Evaluation and Improvement\n",
    "\n",
    "    Cross-Validation 252\n",
    "    Stratified k-Fold Cross-Validation and Other Strategies 254\n",
    "    Grid Search 260\n",
    "    The Danger of Overfitting the Parameters and the Validation Set 261\n",
    "    Grid Search with Cross-Validation 263\n",
    "    Evaluation Metrics and Scoring 275\n",
    "    Keep the End Goal in Mind 275\n",
    "    Metrics for Binary Classification 276\n",
    "    Metrics for Multiclass Classification 296\n",
    "    Regression Metrics 299\n",
    "    Using Evaluation Metrics in Model Selection 300\n",
    "    Summary and Outlook 302\n",
    "\n",
    "6. Algorithm Chains and Pipelines\n",
    "\n",
    "    Parameter Selection with Preprocessing 306\n",
    "    Building Pipelines 308\n",
    "    Using Pipelines in Grid Searches 309\n",
    "    The General Pipeline Interface 312\n",
    "    Convenient Pipeline Creation with make_pipeline 313\n",
    "    Accessing Step Attributes 314\n",
    "    Accessing Attributes in a Grid-Searched Pipeline 315\n",
    "    Grid-Searching Preprocessing Steps and Model Parameters 317\n",
    "    Grid-Searching Which Model To Use 319\n",
    "    Summary and Outlook 320\n",
    "---\n",
    "\n",
    "### **5. Dive into Leep Learning**\n",
    "\n",
    "* 3 Linear Neural Networks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}